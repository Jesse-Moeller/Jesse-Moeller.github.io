I"@<h1 id="selecting-the-project">Selecting the Project</h1>

<p>In the Fall of 2019 I applied for an NSF sponsored summer 2020 internship with the <a href="https://orise.orau.gov/nsf-msgi/">MSGI program</a>. The purpose of this program is to give graduate students an opportunity to solve “real world” problems, regardless of whether the student plans to have an academic or nonacademic career. It is a purely educational opportunity for the benefit of the student. The application process is pretty simple; you tell them why you want the internship, what you’re capable of, and you select a few projects on their page which you think sound interesting. I wanted to learn more about machine learning by working on an interesting problem, and I wanted to gain more experience doing work in a non-academic setting, so this seemed like the perfect fit for me. In a few more words, that’s what I put on my application. I was overjoyed when I recieved the acceptance email in March. Not only did I win a competetive award, but the project I was assigned to was my top choice. The project was titled “Improving Nuclear Data Estimates and Uncertainties Using Machine Learning” and this is the description that the mentors provided:</p>

<blockquote>
  <p>The goal of this project is to utilize machine learning methods to improve the quality of estimates of nuclear reaction cross sections and their uncertainties in nuclear databases.  These nuclear data are critical for understanding and modeling nuclear physics in reactors and other scientific applications. These estimates are obtained using a statistical combination of complex nuclear physics models and experiments. They are then tested in the simulation of validation experiments, which integrate many sets of nuclear data into one model of a complex experiment. Using machine learning, we have been able to identify previously unidentified relationships between nuclear data estimates and benchmark bias. This project will focus on further advancing the methodology for machine-learning-augmented search for sources of bias in benchmarks and basic nuclear physics experiments to improve nuclear data evaluation.</p>
</blockquote>

<p>I had a roughly 45 minute phone conversation with my (main) project mentor, Denise Neudecker, where she gave me a more in-depth description of the project. Denise described what has been done already, what her vision for this summer looked like, and what would be expected of me. There was a lot of jargon that I did not understand in that phone call, but I was able to understand the big picture and I was confident I could fill in the blanks. At this point in time, I did not really know anything about machine learning or nuclear physics. I have lots of programming experience, which was my selling point, but in my heart I felt that I was asking them to trust that I would catch up and learn some material on my own before the program’s start date.</p>

<h1 id="preparation">Preparation</h1>

<p>I love the internet. There is no shortage of free learning material on the internet. I learned calculus on the internet. Any meaningful progress I have made towards learning a computer skill has happened on the internet. Naturally, the internet would be there to help me get into machine learning. There are several dozen starting points available to get into machine learning from ground zero. After reviewing several books, and at the suggestion of my other project mentor, Mike Grosskopf, I decided to crack open Elements of Statistical Learning (ESL), by Friedman, Hastie, and Tibshirani. The book is freely availabe <a href="https://web.stanford.edu/~hastie/ElemStatLearn/">here</a>. I really appreciate the way this book is layed out: there’s a fundamental block of chapters which are, more or less, required by the others, but the rest of the book is a buffet of content and can be consumed in any order. I was encouraged by one of my project mentors to read through Chapter 3, one of those fundamental chapters, and to try and learn about shrinkage methods.</p>

<p>Chapter 3 of ESL is about linear regression. When we want to predict a value $y$, like someone’s weight, based on certain features $x$, like how old that person is, then a simpe way to do it is to just use a linear equation: $\hat{y}=mx+b$. So our model is determined by $m$ and $b$. I guess, in this context, $b$ might mean “the average weight of a newborn”. But how <em>good</em> is that prediction? A straightforward way to judge this is just to add up the error of our predictions. But, to avoid the positive error and negative error cancelling eachother out, we will add up the squared error. We call this the loss function $L(m,b)$, and based on the description above we have</p>

\[L(m,b) = \sum (\hat{y}-y)^2\]

<p>If you know some calculus, then you can just minimize this equation to find the best $m$ and $b$ for the job. If we are predicting a value using more factors than just one, not much changes. The $x$ and $m$ and $b$ become vectors and that’s it. Still we can ask, how <em>good</em> is this model? Sure, objectively it is the minimum of this loss function, but there are other concerns. What if we have several predictors and we don’t believe that each of these will contribute to the prediction? Then it is possible that we can get better prediction accuracy by shrinking some of the coefficients in $m$, or setting them to zero entirely. One way to penalize $m$ for having too many non-zero coefficients is to modify our loss function. Perhaps something like</p>

\[L(m,b) = \underbrace{\sum (\hat{y}-y)^2}_{\text{penalty for being far away from data}} + \underbrace{\lambda ||m||^2}_{\text{penalty for too many coefficients}}\]

<table>
  <tbody>
    <tr>
      <td>Like in the previous situation, since this function is nice, you cand do some calculus to find a minimum. But if we penalize with $</td>
      <td> </td>
      <td>m</td>
      <td> </td>
      <td>$ instead of $</td>
      <td> </td>
      <td>m</td>
      <td> </td>
      <td>^2$ in our loss function, then there is no closed form expression for the minimum. We can still find solutions, however, and in theory both of these models should reduce the number of coefficients that we see in $m$. The three models I have mentioned so far are called linear regression, ridge regression, and lasso regression. There’s another linear model which I won’t explain in detail called elastic net and you can kind of think of it as somewhere between ridge and lasso.</td>
    </tr>
  </tbody>
</table>

<p>With this grounding, I wanted to see how shrinkage could be applied. I downloaded a Major League Baseball dataset and I wanted to predict a player’s weight from the other features in the dataset. The features at my disposal were the player’s team, their position, and their height, weight, and age. With each of these models I wanted to train the model on some of the data set and see how well it behaved on the remaining portion of data that it hasn’t seen. This is standard practice during model selection. Here’s what that code looks like:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">from</span> <span class="nn">pandas</span> <span class="kn">import</span> <span class="n">read_csv</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">ElasticNetCV</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LassoCV</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">RidgeCV</span>
<span class="kn">import</span> <span class="nn">matplotlib</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">plt</span><span class="p">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s">"figure.figsize"</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>

<span class="c1">#Import MLB player weight data
</span><span class="n">names</span> <span class="o">=</span> <span class="p">[</span><span class="s">'Name'</span><span class="p">,</span><span class="s">'Team'</span><span class="p">,</span><span class="s">'Position'</span><span class="p">,</span><span class="s">'Height'</span><span class="p">,</span><span class="s">'Weight'</span><span class="p">,</span><span class="s">'Age'</span><span class="p">]</span>
<span class="n">MLB</span> <span class="o">=</span> <span class="n">read_csv</span><span class="p">(</span><span class="s">r"C:\Users\moell\Desktop\Machine Learning\Data\MLBweight.csv"</span><span class="p">,</span> <span class="n">names</span><span class="o">=</span><span class="n">names</span><span class="p">)</span>
<span class="n">MLB</span><span class="o">=</span><span class="n">MLB</span><span class="p">.</span><span class="n">drop</span><span class="p">(</span><span class="s">'Name'</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1">#one-hot encoding
</span><span class="n">MLB</span><span class="o">=</span><span class="n">pd</span><span class="p">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">MLB</span><span class="p">)</span>

<span class="c1"># Formatting X and y
</span><span class="n">y</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">MLB</span><span class="p">[</span><span class="s">'Weight'</span><span class="p">])</span>
<span class="n">MLB</span><span class="o">=</span><span class="n">MLB</span><span class="p">.</span><span class="n">drop</span><span class="p">(</span><span class="s">'Weight'</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">feature_list</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">MLB</span><span class="p">.</span><span class="n">columns</span><span class="p">)</span>
<span class="n">X</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">MLB</span><span class="p">)</span>
<span class="n">y</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

<span class="c1">#Splitting data
</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1">#Make a list of labelled models (name,model)
</span><span class="n">models</span><span class="o">=</span><span class="p">[(</span><span class="s">'Linear Regression'</span><span class="p">,</span> <span class="n">LinearRegression</span><span class="p">()),</span>
        <span class="p">(</span><span class="s">'Ridge'</span><span class="p">,</span> <span class="n">RidgeCV</span><span class="p">()),</span>
        <span class="p">(</span><span class="s">'Lasso'</span><span class="p">,</span> <span class="n">LassoCV</span><span class="p">(</span><span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)),</span>
        <span class="p">(</span><span class="s">'Elastic Net'</span><span class="p">,</span> <span class="n">ElasticNetCV</span><span class="p">(</span><span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">))]</span>

<span class="c1">#Compare Results
</span><span class="k">for</span> <span class="p">(</span><span class="n">name</span><span class="p">,</span><span class="n">model</span><span class="p">)</span> <span class="ow">in</span> <span class="n">models</span><span class="p">:</span>
    <span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">errors</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">predictions</span> <span class="o">-</span> <span class="n">y_test</span><span class="p">)</span>
        
    <span class="k">print</span><span class="p">(</span><span class="s">'Mean Absolute Error for'</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="s">' :'</span><span class="p">,</span> <span class="nb">round</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">errors</span><span class="p">),</span> <span class="mi">2</span><span class="p">))</span>
    <span class="n">coefs</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s">'Feature'</span><span class="p">:</span> <span class="n">feature_list</span><span class="p">,</span>
                      <span class="s">'Importance'</span><span class="p">:</span> <span class="n">model</span><span class="p">.</span><span class="n">coef_</span><span class="p">})</span>
    <span class="n">coefs</span> <span class="o">=</span> <span class="n">coefs</span><span class="p">.</span><span class="n">set_index</span><span class="p">(</span><span class="s">'Feature'</span><span class="p">).</span><span class="n">sort_values</span><span class="p">(</span><span class="s">'Importance'</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="n">coefs_plot</span> <span class="o">=</span> <span class="n">coefs</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s">'bar'</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span> <span class="n">name</span><span class="o">+</span><span class="s">' feature importance'</span><span class="p">)</span>
</code></pre></div></div>

<p>And here’s the output:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Mean Absolute Error for Linear Regression  : 12.64
Mean Absolute Error for Ridge  : 12.55
Mean Absolute Error for Lasso  : 12.53
Mean Absolute Error for Elastic Net  : 12.58
</code></pre></div></div>

<p>Also, here are the plots of the features together with their importance:</p>

<p><img src="/assets/images/mlb_weight_lr.png" alt="image" /></p>
:ET