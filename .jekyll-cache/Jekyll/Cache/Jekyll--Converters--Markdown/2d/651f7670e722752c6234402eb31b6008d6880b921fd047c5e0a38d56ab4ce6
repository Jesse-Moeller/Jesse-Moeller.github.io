I"∑<p>This summer I had the great fortune of interning at Los Alamos National Lab with two staff scientists from the XCP-5 team. The internship was supported by the NSF-MSGI program, which I applied for in Fall 2019. Here is the description of the work we set out to do, provided by the program website:</p>

<blockquote>
  <p>The goal of this project is to utilize machine learning methods to improve the quality of estimates of nuclear reaction cross sections and their uncertainties in nuclear databases.  These nuclear data are critical for understanding and modeling nuclear physics in reactors and other scientific applications. These estimates are obtained using a statistical combination of complex nuclear physics models and experiments. They are then tested in the simulation of validation experiments, which integrate many sets of nuclear data into one model of a complex experiment. Using machine learning, we have been able to identify previously unidentified relationships between nuclear data estimates and benchmark bias. This project will focus on further advancing the methodology for machine-learning-augmented search for sources of bias in benchmarks and basic nuclear physics experiments to improve nuclear data evaluation.</p>
</blockquote>

<p>Due to COVID-19 restrictions, the internship was almost cancelled. I consider myself lucky that the internship was permitted to continue in a fully remote setting. I learned about nuclear physics, nuclear data, machine learning, and I also learned about life as a staff scientist at Los Alamos.</p>

<h1 id="why-machine-learning-in-nuclear-data">Why Machine Learning in Nuclear Data?</h1>

<p>As mentioned above in the description, it is hard to overstate how important it is to have quality estimates of nuclear reaction cross sections and their associated uncertainties. My main project mentor, Denise Neudecker, was interested in searching for sources of bias in experimental nuclear data values. Allow me to unpack that jargon a little bit. There are lots of scientists doing experiments all the time. They get some money from a grant to shoot neutrons at some material in a highly controlled setting. They record what happens, and the product of their labor is referred to as experimental nuclear data. These experiments are not all the same; they can differ in big ways like which material is used, and how much energy is used. But, they can also differ in more subtle ways, like all the different kinds of equipment that could be used or which methods they use for measuring different quantities (even if they are theoretically measuring the same thing). Thanks to previous work at LANL, we already had a clue that these secondary features can play a role in identifying previously unidentified bias in the data.</p>

<p>Last summer, in 2019, the authors of this paper used support vector machines to classify experimental nuclear data as outliers or not. From there, different techniques were used to associate experiment features with the probability of being an outlier. While being interesting and conclusive work of its own, this was also a proof of concept that nuclear data evaluation can be positively augmented with machine learning techniques. Rather than classifying and diagnosing outliers, the goal of our work this summer was to model bias for each data point. That is, we assume that each experimental data value is affected by the features present in the experiment, and then we look for those features which are most responsible for bias.</p>

<p>Experimental data are analyzed in detail and sometimes over many decades, by many different scientists and in different ways, before they are incorporated into a nuclear-data evaluation. Along the way, experimental data might be rejected or down-weighted by increased uncertainties due to a perceived systematic discrepancy, as determined by expert opinion, that cannot be readily assessed with regards to the discrepancies‚Äô origin. The remaining experimental data and modified covariances are then evaluated with a standard generalized least squares algorithm (GLLS). The expert judgment resulting in rejection or increased uncertainties of nuclear data is often made on the basis of a combination of perceived discrepancy in mean values as well as familiarity with features (that are aspects of the measurement such as detector type, sample mass, <em>etc.</em>) in the experiment. However, careful analysis of these experiments reveals a space of features often too large for one person to account for (up to a hundred per data set), and this is where expert opinion may not be the best option. Machine learning, on the other hand, can be useful for detecting complex inter-dependencies in a large feature space.</p>

<h1 id="the-model-augmented-glls">The Model: Augmented GLLS</h1>

<p>The first step of the project was to decide which machine learning techniques would be best for solving this problem. Since we were trying to find corrections for systematically discrepant experimental data based on the experiment features, we would also need to identify when such data were discrepant in the <em>first place</em>. How is it possible to detect a deviation from the true mean value if the true mean values are, in some sense, exactly what we are looking for already? In light of this line of questioning, we decided to move beyond a model which classifies data as outlying given a fixed nuclear-data estimate in favor of estimating the nuclear data and potential systematic bias concurrently. To do so, we built a machine learning model which assumes that each reported experimental mean value may be offset from the ‚Äútrue‚Äù value by some bias due to error related to the experiment‚Äôs features in addition to measurement error. While there are a large number of potential sources of bias, we utilize sparsity-inducing regularization to enforce the estimation of bias with a minimal set of features.</p>

<p>To understand the machine learning model we developed, we will first discuss the GLLS algorithm. The algorithm takes as input experimental values $y$ and their associated covariances $V_y$, prior values $p_a$ and their associated covariances $V_a$. Both $y$ and $p_a$ represent vectors of a physical quantity, viewed as a random variable depending incident- and outgoing-neutron energy in MeV, captured at several different energies. The experimental values $y$ are taken at many different energy values and ranges across experiments, as each experimentalist measures only those specific ranges of interest to and accessible by the equipment of their respective project. The prior data, on the other hand, represent curated values on a fixed scale of energy. Prior data can be supplied either by a nuclear model or by a previous evaluation. We linearly interpolate the prior data so that they are on the same scale as the experimental data using the matrix $A$. That is, $A\cdot p_a$ is on the same energy scale as $y$. Now, given any proposed mean values (on the same scale as the prior) $p$, we assess how ‚Äúbad‚Äù these proposed mean values are by comparing them to both the prior and the experimental data. We record these deviations in the loss function in equation below, with the second equation being the same as the first, but with more convenient notation.
\(\begin{align}
L(p) &amp;= \underbrace{(y - A\cdot p)^T V_y^{-1}(y - A\cdot p)}_{\text{deviation from experiments}} \ + \underbrace{(p_a-p)^T V_a^{-1}(p_a-p)}_{\text{deviation from established prior values}}\\
&amp; = ||y - A\cdot P||_{V_y}^2 + ||p_a - p||_{V_a}^2.
\end{align}\)</p>
:ET