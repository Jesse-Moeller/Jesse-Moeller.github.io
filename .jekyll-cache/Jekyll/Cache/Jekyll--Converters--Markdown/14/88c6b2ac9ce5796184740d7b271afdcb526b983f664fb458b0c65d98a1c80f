I"<h1 id="selecting-the-project">Selecting the Project</h1>

<p>In the Fall of 2019 I applied for an NSF sponsored summer 2020 internship with the <a href="https://orise.orau.gov/nsf-msgi/">MSGI program</a>. The purpose of this program is to give graduate students an opportunity to solve “real world” problems, regardless of whether the student plans to have an academic or nonacademic career. It is a purely educational opportunity for the benefit of the student. The application process is pretty simple; you tell them why you want the internship, what you’re capable of, and you select a few projects on their page which you think sound interesting. I wanted to learn more about machine learning by working on an interesting problem, and I wanted to gain more experience doing work in a non-academic setting, so this seemed like the perfect fit for me. In a few more words, that’s what I put on my application. I was overjoyed when I recieved the acceptance email in March. Not only did I win a competetive award, but the project I was assigned to was my top choice. The project was titled “Improving Nuclear Data Estimates and Uncertainties Using Machine Learning” and this is the description that the mentors provided:</p>

<blockquote>
  <p>The goal of this project is to utilize machine learning methods to improve the quality of estimates of nuclear reaction cross sections and their uncertainties in nuclear databases.  These nuclear data are critical for understanding and modeling nuclear physics in reactors and other scientific applications. These estimates are obtained using a statistical combination of complex nuclear physics models and experiments. They are then tested in the simulation of validation experiments, which integrate many sets of nuclear data into one model of a complex experiment. Using machine learning, we have been able to identify previously unidentified relationships between nuclear data estimates and benchmark bias. This project will focus on further advancing the methodology for machine-learning-augmented search for sources of bias in benchmarks and basic nuclear physics experiments to improve nuclear data evaluation.</p>
</blockquote>

<p>I had a roughly 45 minute phone conversation with my (main) project mentor, Denise Neudecker, where she gave me a more in-depth description of the project. Denise described what has been done already, what her vision for this summer looked like, and what would be expected of me. There was a lot of jargon that I did not understand in that phone call, but I was able to understand the big picture and I was confident I could fill in the blanks. At this point in time, I did not really know anything about machine learning or nuclear physics. I have lots of programming experience, which was my selling point, but in my heart I felt that I was asking them to trust that I would catch up and learn some material on my own before the program’s start date.</p>

<h1 id="preparation">Preparation</h1>

<p>I love the internet. There is no shortage of free learning material on the internet. I learned calculus on the internet. Any meaningful progress I have made towards learning a computer skill has happened on the internet. Naturally, the internet would be there to help me get into machine learning. There are several dozen starting points available to get into machine learning from ground zero. After reviewing several books, and at the suggestion of my other project mentor, Mike Grosskopf, I decided to crack open Elements of Statistical Learning (ESL), by Friedman, Hastie, and Tibshirani. The book is freely availabe <a href="https://web.stanford.edu/~hastie/ElemStatLearn/">here</a>. I really appreciate the way this book is layed out: there’s a fundamental block of chapters which are, more or less, required by the others, but the rest of the book is a buffet of content and can be consumed in any order. I was encouraged by one of my project mentors to read through Chapter 3, one of those fundamental chapters, and to try and learn about shrinkage methods.</p>

<p>Chapter 3 of ESL is about linear regression. When we want to predict a value $y$, like someone’s weight, based on certain features $x$, like how old that person is, then a simpe way to do it is to just use a linear equation: $\hat{y}=mx+b$. So our model is determined by $m$ and $b$. I guess, in this context, $b$ might mean “the average weight of a newborn”. But how <em>good</em> is that prediction? A straightforward way to judge this is just to add up the error of our predictions. But, to avoid the positive error and negative error cancelling eachother out, we will add up the squared error. We call this the loss function $L(m,b)$, and based on the description above we have</p>

\[L(m,b) = \sum (\hat{y}-y)^2\]

<p>If you know some calculus, then you can just minimize this equation to find the best $m$ and $b$ for the job.let’s To further investigate the ideas of Chapter 3, I wanted to get my hands on some data and try this stuff out for myself. It was clear to me that getting familiar with the scikit-learn and pandas python modules would be very helpful, if not necessary, so this was a good opportunity to try those out. I installed conda with Spyder and worked through a few tutorials that I found. Eventually, I felt ready to try out my own dataset. I downloaded a Major League Baseball database and decided to build a model that would predict a player’s weight from the other features present in the data:name, team, position, height, and age. Keeping in line with the ideas from Chapter 3, I used only linear models.</p>

<p>Chapter 3</p>
:ET