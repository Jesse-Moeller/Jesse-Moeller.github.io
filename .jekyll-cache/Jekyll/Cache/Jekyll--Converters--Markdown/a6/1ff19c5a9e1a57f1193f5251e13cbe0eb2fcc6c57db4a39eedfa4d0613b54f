I"=<p>This summer I had the great fortune of interning at Los Alamos National Lab with two staff scientists from the XCP-5 team. The internship was supported by the NSF-MSGI program, which I applied for in Fall 2019. Here is the description of the work we set out to do, provided by the program website:</p>

<blockquote>
  <p>The goal of this project is to utilize machine learning methods to improve the quality of estimates of nuclear reaction cross sections and their uncertainties in nuclear databases.  These nuclear data are critical for understanding and modeling nuclear physics in reactors and other scientific applications. These estimates are obtained using a statistical combination of complex nuclear physics models and experiments. They are then tested in the simulation of validation experiments, which integrate many sets of nuclear data into one model of a complex experiment. Using machine learning, we have been able to identify previously unidentified relationships between nuclear data estimates and benchmark bias. This project will focus on further advancing the methodology for machine-learning-augmented search for sources of bias in benchmarks and basic nuclear physics experiments to improve nuclear data evaluation.</p>
</blockquote>

<p>Due to COVID-19 restrictions, the internship was almost cancelled. I consider myself lucky that the internship was permitted to continue in a fully remote setting. I learned about nuclear physics, nuclear data, machine learning, and I also learned about life as a staff scientist at Los Alamos.</p>

<h1 id="why-machine-learning-in-nuclear-data">Why Machine Learning in Nuclear Data?</h1>

<p>As mentioned above in the description, it is hard to overstate how important it is to have quality estimates of nuclear reaction cross sections and their associated uncertainties. My main project mentor, Denise Neudecker, was interested in searching for sources of bias in experimental nuclear data values. Allow me to unpack that jargon a little bit. There are lots of scientists doing experiments all the time. They get some money from a grant to shoot neutrons at some material in a highly controlled setting. They record what happens, and the product of their labor is referred to as experimental nuclear data. These experiments are not all the same; they can differ in big ways like which material is used, and how much energy is used. But, they can also differ in more subtle ways, like all the different kinds of equipment that could be used or which methods they use for measuring different quantities (even if they are theoretically measuring the same thing). Thanks to previous work at LANL, we already had a clue that these secondary features can play a role in identifying previously unidentified bias in the data.</p>

<p>Last summer, in 2019, the authors of this paper used support vector machines to classify experimental nuclear data as outliers or not. From there, different techniques were used to associate experiment features with the probability of being an outlier. While being interesting and conclusive work of its own, this was also a proof of concept that nuclear data evaluation can be positively augmented with machine learning techniques. Rather than classifying and diagnosing outliers, the goal of our work this summer was to model bias for each data point. That is, we assume that each experimental data value is affected by the features present in the experiment, and then we look for those features which are most responsible for bias.</p>

<p>Experimental data are analyzed in detail and sometimes over many decades, by many different scientists and in different ways, before they are incorporated into a nuclear-data evaluation. Along the way, experimental data might be rejected or down-weighted by increased uncertainties due to a perceived systematic discrepancy, as determined by expert opinion, that cannot be readily assessed with regards to the discrepancies’ origin. The remaining experimental data and modified covariances are then evaluated with a standard generalized least squares algorithm (GLLS). The expert judgment resulting in rejection or increased uncertainties of nuclear data is often made on the basis of a combination of perceived discrepancy in mean values as well as familiarity with features (that are aspects of the measurement such as detector type, sample mass, <em>etc.</em>) in the experiment. However, careful analysis of these experiments reveals a space of features often too large for one person to account for (up to a hundred per data set), and this is where expert opinion may not be the best option. Machine learning, on the other hand, can be useful for detecting complex inter-dependencies in a large feature space.</p>

<h1 id="the-model-augmented-glls">The Model: Augmented GLLS</h1>

<p>The first step of the project was to decide which machine learning techniques would be best for solving this problem. Since we were trying to find corrections for systematically discrepant experimental data based on the experiment features, we would also need to identify when such data were discrepant in the <em>first place</em>. How is it possible to detect a deviation from the true mean value if the true mean values are, in some sense, exactly what we are looking for already? In light of this line of questioning, we decided to move beyond a model which classifies data as outlying given a fixed nuclear-data estimate in favor of estimating the nuclear data and potential systematic bias concurrently. To do so, we built a machine learning model which assumes that each reported experimental mean value may be offset from the “true” value by some bias due to error related to the experiment’s features in addition to measurement error. While there are a large number of potential sources of bias, we utilize sparsity-inducing regularization to enforce the estimation of bias with a minimal set of features.</p>

<p>To understand the machine learning model we developed, we will first discuss the GLLS algorithm. The algorithm takes as input experimental values $y$ and their associated covariances $V_y$, prior values $p_a$ and their associated covariances $V_a$. Both $y$ and $p_a$ represent vectors of a physical quantity, viewed as a random variable depending incident- and outgoing-neutron energy in MeV, captured at several different energies. The experimental values $y$ are taken at many different energy values and ranges across experiments, as each experimentalist measures only those specific ranges of interest to and accessible by the equipment of their respective project. The prior data, on the other hand, represent curated values on a fixed scale of energy. Prior data can be supplied either by a nuclear model or by a previous evaluation. We linearly interpolate the prior data so that they are on the same scale as the experimental data using the matrix $A$. That is, $A\cdot p_a$ is on the same energy scale as $y$. Now, given any proposed mean values (on the same scale as the prior) $p$, we assess how “bad” these proposed mean values are by comparing them to both the prior and the experimental data. We record these deviations in the loss function in equation below, with the second equation being the same as the first, but with more convenient notation.</p>

\[\begin{align}
L(p) &amp;= \underbrace{(y - A\cdot p)^T V_y^{-1}(y - A\cdot p)}_{\text{deviation from experiments}} \ + \underbrace{(p_a-p)^T V_a^{-1}(p_a-p)}_{\text{deviation from established prior values}}\\
&amp; = ||y - A\cdot P||_{V_y}^2 + ||p_a - p||_{V_a}^2.
\end{align}\]

<p>With the GLLS loss function, one can use basic calculus to find a closed form for $p$ which minimizes $L$. To incorporate the experiment features into this loss function, we one-hot encode the features into a matrix $X$. Each row corresponds to a specific measurement, and the columns record the presence of an experiment feature. For instance, if a specific detector type is used for an experiment where one hundred data points are collected then we would see one hundred rows which have a value “1” in a specific column corresponding to this detector type. Our assumption is that $y = p + X\beta + \epsilon$, where $\beta$ is a vector (one entry per experiment feature) of bias terms and $\epsilon$ is random noise. In other words, we assume that what is recorded in the experiment is the true nuclear-data observable, offest by bias due to features present in the experiment, and with some stochastic corruption due to measurement error which we cannot control. These assumptions motivate a new loss function:</p>

\[\begin{align}
L_\lambda(p,\beta) &amp;= ||y - A\cdot P - X\cdot \beta||_{V_y}^2 + ||p_a - p||_{V_a}^2 + \lambda||\beta||_{L_1}.
\end{align}\]

<table>
  <tbody>
    <tr>
      <td>The first two terms have the same interpretation as in the GLLS equation, modulo correcting for bias with $X\beta$, and the final term is a regularization term which helps us to achieve sparse coefficients in $\beta$. Because of the large number of features, there are many degrees of freedom to over-fit random error in an unconstrained model for accounting for bias. Fitting that model, will then allow $\lambda</td>
      <td> </td>
      <td>\beta</td>
      <td> </td>
      <td>_{l_1}$ to become large in order to utilize those degrees of freedom. Even worse, in the case that the number of features is greater than the number of observations, the problem is unconstrained and the model can interpolate the noise in the experimental observations. By imposing the regularization term, we can constrain the degrees of freedom to avoid this overfitting. Better $\beta$ values (which are more likely to minimize this loss function) are ones which have more specificity. By choosing $L_1$ regularization, large values of $\lambda$ will lead to an optimal fit with many values of $\beta$ estimated to precisely zero. The term $\lambda$ is chosen by a process called cross-validation, which chooses the $\lambda$ we find most capable of predicting data that `“hasn’t seen yet”.</td>
    </tr>
  </tbody>
</table>

<p>Much of the rest of the project was spent on the equally necessary step of preparing data and fitting the model. The available data requires several rounds of reformatting before feeding them into our machine learning algorithm. Even then, the loss function that we defined above (unlike GLLS) does not have a closed-form minimum. Thus a considerable amount of time was also spent on searching for good optimization techniques to find the minimum value. The first optimization technique we tried was a slightly modified version of proximal gradient descent with nesterov acceleration. The results of our implementation were questionable; while the loss function in equation was being minimized, the $L_1$ regularization did not seem to be in effect. We believe that there was some instability in the gradient step, and it is possible that further tuning would resolve the issue. In contrast, coordinate descent optimization was easy to implement due to the closed form solution for $p$ given a fixed $\beta$, the $L_1$ regularization seemed to be in effect, and the algorithm required a comparable number of iterations.</p>

<h1 id="contributions-to-the-project">Contributions to the Project</h1>

<p>At the beginning of the internship, my mentors presented to me a proposal for the work that we would complete this summer. It was thorough, detailed, optimistic in addition to being easily modifiable. I agreed with the proposed work, and that is where we started out. I played a smaller role in the model-selection phase, deferring heavily to my second mentor, Mike Grosskopf, a statistician and machine learning specialist, though my input was welcome and well-received. My contributions for this project fall into two categories. First, I wrote many lines of code pulling in the nuclear data and the experimental features and formatting them so that they could be plugged into the machine learning. This was necessary and somewhat difficult, as the data that we used used for our model came from three different physical quantities, each of which has a different treatment in how the data are stored and formatted. Second, I spent a lot of time developing the actual machine learning code itself. This includes searching for good optimization techniques, writing my own cross-validation code, tuning the descent parameters, and plotting the resulting figures. At the very end of the internship, I presented our findings to the Nuclear Data Machine Learning research group in the form of a 25 minute slide presentation over a conference call.</p>

<h1 id="skills-and-knowledge-gained">Skills and Knowledge Gained</h1>

<p>I will start this section by first talking about skills and knowledge that I did not expect to gain from this internship. In the early spring of 2020, the world was struck by the COVID-19 epidemic. I was lucky enough that my internship experience was permitted to continue remotely for the summer. This presented unique challenges, but also valuable lessons. Through this  experience, due to the security and computing requirements, I have become more familiar with computing in remote UNIX environments, the CONDA package environment, and the PuTTY tool. On the human side, I also learned quite a bit from the staff scientists about working remotely as a team, thanks to their model professionalism and organized conduct.</p>

<p>I am happy to report that I am satisfied with the level of development of technical skill that I was afforded to achieve this summer. I purposefully sought out internships with a focus in applying machine learning techniques. By the guidance of my mentors, and materials suggested by them, I have learned a lot about machine learning in a short amount of time - even about machine learning techniques that we did not end up using. I became very familiar with the scikit-learn and pandas packages which will be useful in the future, regardless of which career paths I decide to pursue.</p>

<p>In the presence of interesting work and encouraging mentors, overcoming the difficulty of learning a new skill-set is a rewarding process. This was certainly the case for me this summer. I was very glad to be working with real experimental nuclear data under the guidance of staff scientists. I also learned a lot about nuclear physics, the nuclear-data evaluation process, and more about the scientific process in general.</p>

<h1 id="impact-on-career">Impact on Career</h1>

<p>The experience I had at LANL has affected my interest in machine learning in a positive way. On the outset, my interest in machine learning was firmly rooted in the wow-factor of the product and the marketability of the skill set. While developing machine learning algorithms, however, I found myself very attracted to all of the different mathematics involved and I was genuinely curious about the scientific consequences of our results. I enjoyed thinking about the problem of model selection, trying out the different optimization techniques, and also the software engineering aspect of the work; it is very satisfying to write clean working code, and even more satisfying when your team members find that code legible and useful. I am honored by the idea that my code might assist in the efforts to achieve higher quality evaluated nuclear data. This internship has definitely affected how I will conduct my job search in the future; I will be be focusing on those opportunities in which I believe I will be most stimulated and challenged in my work.</p>
:ET